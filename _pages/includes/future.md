<span class='anchor' id='future'></span>

# ðŸ”® Future Research Directions

While Transformer-based architectures have dominated recent years, I see substantial room for progress across several directions:

### 1. Fundamental Model Enhancement

Despite extensive progress in both academia and industry, key opportunities remain for improving foundation models:

- **Capability Gaps**: Create targeted benchmarks to expose humanâ€“AI gaps, then close them with explainable methods:
  - Optimize layer and head interactions
  - Introduce interpretable decoding control
  - Enable small models to compete with larger ones (Reasoning)
  - Orchestrate efficient interactions between reasoning and nonâ€‘reasoning modules

- **Training vs. Inference**: Inference-time adaptations are effective, but scaling during training will finally surpass and substitute them. As resources allow, I will shift emphasis from inference-time tweaks to training-time optimization.

- **Interpretability for Innovation**: Use interpretability not only to explain but to improve training. For example, insights from the attentionâ€‘sink mechanism (since 2022) have informed KVâ€‘cache optimization, extensions to VLMs, and quantization-aware training.

### 2. Multimodal Evolution

As LLM research has outpaced multimodal progress, I am especially interested in:

- Identifying and addressing limitations in current MLLM architectures
- Developing more efficient architectures for processing multimodal information, considering visual redundancy and modality alignment challenges
- Exploring novel architectural paradigms beyond current conventions

### 3. AI for Scientific Discovery

The next frontier is applying LLMs to scientific discovery, with focus on:

- **Problem Identification**: Discover valuable new application areas as LLM capabilities expand
- **Targeted Solutions**: Adapt and optimize models for specific scientific domains
- **Evaluation Framework**: Tackle problems through multiple lenses:
  - Unknown problems (benchmark construction)
  - Known problems with:
    * Simple evaluation but challenging solutions (effective methods) or highâ€‘cost evaluation (efficiency)
    * Easy solutions but complex evaluation requirements (e.g., RLVR)
